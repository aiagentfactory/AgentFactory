# TrainingFactory - è®­ç»ƒå·¥å‚

åŸºäº [Simple-Efficient/RL-Factory](https://github.com/Simple-Efficient/RL-Factory) çš„ä¼˜ç§€ç»éªŒå¢å¼º

## ğŸ“‹ åŠŸèƒ½æ¦‚è¿°

TrainingFactoryæ˜¯Agent Factoryçš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ä»æ•°æ®åˆ°æ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼Œç‰¹åˆ«é’ˆå¯¹AI Agentçš„Tool-callingå’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ“ **SFTç›‘ç£å¾®è°ƒ**: åŸºç¡€èƒ½åŠ›è®­ç»ƒ
- ğŸ® **RLå¼ºåŒ–å­¦ä¹ **: PPOã€DPOã€GRPOã€RFT
- ğŸ **å¥–åŠ±å»ºæ¨¡**: Reward Model + Process Reward
- âš¡ **é«˜æ•ˆè®­ç»ƒ**: å¼‚æ­¥å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼ˆ2xé€Ÿåº¦ï¼‰
- ğŸ“š **æ¨¡å‹æ³¨å†Œ**: ç‰ˆæœ¬ç®¡ç†ä¸æ¨¡å‹æ™‹çº§
- ğŸ”§ **å¤šæ¨¡å‹æ”¯æŒ**: Qwen3ã€DeepSeekã€Llamaç­‰

---

## ğŸ¯ ä»RL-Factoryå­¦åˆ°çš„ç»éªŒ

### 1. ç¯å¢ƒè§£è€¦è®¾è®¡ âœ…

**RL-Factoryçš„åšæ³•:**
- å°†ç¯å¢ƒï¼ˆå·¥å…·ã€å¥–åŠ±å‡½æ•°ï¼‰ä¸è®­ç»ƒæ¡†æ¶è§£è€¦
- åªéœ€æä¾›ä¸€ä¸ªå·¥å…·é…ç½®å’Œä¸€ä¸ªå¥–åŠ±å‡½æ•°å³å¯å¼€å§‹è®­ç»ƒ

**åº”ç”¨åˆ°TrainingFactory:**
```python
# ç®€å•çš„è®­ç»ƒé…ç½®
config = TrainingConfig(
    model="Qwen3-4B",
    environment_id="env_search",  # æ¥è‡ªEnvironmentFactory
    reward_function="rule_based",  # è§„åˆ™å¥–åŠ±
    tools=["search", "calculator"]  # MCPå·¥å…·
)

trainer.train(config)
```

### 2. å¼‚æ­¥å¹¶è¡Œå·¥å…·è°ƒç”¨ âš¡

**RL-Factoryçš„ä¼˜åŠ¿:**
- è®­ç»ƒé€Ÿåº¦æå‡2xï¼ˆvs Search-R1ï¼‰
- æ‰¹å¤„ç† + å¼‚æ­¥å¹¶è¡Œå·¥å…·è°ƒç”¨
- AsyncLLMEngineæ”¯æŒ

**åº”ç”¨åˆ°TrainingFactory:**
```python
class AsyncToolExecutor:
    """å¼‚æ­¥å¹¶è¡Œå·¥å…·æ‰§è¡Œå™¨"""
    
    async def batch_execute(self, tool_calls: List[ToolCall]):
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        tasks = [self._execute_tool(call) for call in tool_calls]
        results = await asyncio.gather(*tasks)
        return results
```

### 3.è¿›ç¨‹å¥–åŠ± (Process Reward) ğŸ¯

**RL-Factoryçš„ç†å¿µ:**
- ä¸ä»…å¥–åŠ±æœ€ç»ˆç»“æœï¼Œè¿˜å¥–åŠ±ä¸­é—´æ­¥éª¤
- æ›´å¥½åœ°æŒ‡å¯¼Agentçš„å·¥å…·è°ƒç”¨è¡Œä¸º

**åº”ç”¨åˆ°TrainingFactory:**
```python
def calculate_process_reward(trajectory):
    """è®¡ç®—è¿›ç¨‹å¥–åŠ±"""
    step_rewards = []
    for step in trajectory:
        # æ¯ä¸€æ­¥éƒ½ç»™å¥–åŠ±
        if step.tool_call_correct:
            step_rewards.append(+0.5)
        if step.reasoning_clear:
            step_rewards.append(+0.3)
    
    final_reward = sum(step_rewards) + outcome_reward
    return final_reward
```

### 4. å¤šæ¨¡å‹æ”¯æŒ ğŸ¤–

**RL-Factoryæ”¯æŒ:**
- Qwen3ï¼ˆæ¨èï¼Œæ”¯æŒMCPï¼‰
- Qwen2.5
- DeepSeek
- Llama
- æœªæ¥æ”¯æŒå¤šæ¨¡æ€

**åº”ç”¨åˆ°TrainingFactory:**
```python
SUPPORTED_MODELS = {
    "qwen3-4b": "Qwen/Qwen3-4B-Instruct",
    "qwen3-8b": "Qwen/Qwen3-8B-Instruct",
    "deepseek-v3": "deepseek-ai/DeepSeek-V3",
    "llama3-8b": "meta-llama/Llama-3-8B-Instruct"
}
```

### 5. é«˜æ•ˆçš„å¥–åŠ±è®¡ç®— ğŸ’°

**RL-Factoryçš„æ–¹æ³•:**
- åˆ†å¸ƒå¼éƒ¨ç½²LRMï¼ˆå¦‚QwQ-32Bï¼‰
- å¼‚æ­¥å¹¶è¡Œè®¡ç®—å¥–åŠ±
- å·¥å…·è°ƒç”¨ç»“æœç¼“å­˜

**åº”ç”¨åˆ°TrainingFactory:**
```python
class DistributedRewardModel:
    """åˆ†å¸ƒå¼å¥–åŠ±æ¨¡å‹"""
    
    def __init__(self, model="QwQ-32B", num_workers=4):
        self.workers = [
            RewardWorker(model) for _ in range(num_workers)
        ]
    
    async def compute_rewards(self, trajectories):
        # å¹¶è¡Œè®¡ç®—
        tasks = [
            worker.compute(traj) 
            for worker, traj in zip(self.workers, trajectories)
        ]
        rewards = await asyncio.gather(*tasks)
        return rewards
```

---

## ğŸ“ ç›®å½•ç»“æ„

```
TrainingFactory/
â”œâ”€â”€ README.md                  # æœ¬æ–‡æ¡£
â”œâ”€â”€ setup.py                   # åŒ…é…ç½®
â”œâ”€â”€ requirements.txt           # ä¾èµ–
â”‚
â”œâ”€â”€ services/                  # ä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ sft_trainer.py         # SFTè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ rl_trainer.py          # RLè®­ç»ƒå™¨ï¼ˆPPO/DPO/GRPOï¼‰
â”‚   â”œâ”€â”€ reward_model.py        # å¥–åŠ±æ¨¡å‹
â”‚   â”œâ”€â”€ model_registry.py      # æ¨¡å‹æ³¨å†Œ
â”‚   â”œâ”€â”€ async_executor.py      # å¼‚æ­¥å·¥å…·æ‰§è¡Œå™¨ â­
â”‚   â””â”€â”€ process_reward.py      # è¿›ç¨‹å¥–åŠ± â­
â”‚
â”œâ”€â”€ algorithms/                # è®­ç»ƒç®—æ³•
â”‚   â”œâ”€â”€ ppo.py                 # PPOç®—æ³•
â”‚   â”œâ”€â”€ dpo.py                 # DPOç®—æ³•
â”‚   â”œâ”€â”€ grpo.py                # GRPOç®—æ³• â­
â”‚   â””â”€â”€ rft.py                 # RFTç®—æ³•
â”‚
â”œâ”€â”€ models/                    # æ¨¡å‹é€‚é…
â”‚   â”œâ”€â”€ qwen3.py               # Qwen3æ”¯æŒ â­
â”‚   â”œâ”€â”€ deepseek.py            # DeepSeekæ”¯æŒ
â”‚   â””â”€â”€ llama.py               # Llamaæ”¯æŒ
â”‚
â”œâ”€â”€ tools/                     # å·¥å…·é›†æˆ
â”‚   â”œâ”€â”€ mcp_tools.py           # MCPå·¥å…·æ”¯æŒ â­
â”‚   â”œâ”€â”€ custom_tools.py        # è‡ªå®šä¹‰å·¥å…·
â”‚   â””â”€â”€ tool_cache.py          # å·¥å…·ç¼“å­˜ â­
â”‚
â”œâ”€â”€ rewards/                   # å¥–åŠ±ç³»ç»Ÿ
â”‚   â”œâ”€â”€ rule_based.py          # è§„åˆ™å¥–åŠ±
â”‚   â”œâ”€â”€ model_judge.py         # æ¨¡å‹åˆ¤æ–­
â”‚   â””â”€â”€ process_reward.py      # è¿›ç¨‹å¥–åŠ± â­
â”‚
â”œâ”€â”€ configs/                   # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ qwen3_grpo.yaml        # Qwen3 GRPOé…ç½®
â”‚   â””â”€â”€ templates/             # é…ç½®æ¨¡æ¿
â”‚
â”œâ”€â”€ tests/                     # æµ‹è¯•
â””â”€â”€ docs/                      # æ–‡æ¡£
    â””â”€â”€ GRPO_GUIDE.md          # GRPOä½¿ç”¨æŒ‡å—
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
cd TrainingFactory
pip install -e .

# å®‰è£…RLä¾èµ–
pip install -e ".[rl]"
```

### ç®€å•ç¤ºä¾‹ - SFTè®­ç»ƒ

```python
from TrainingFactory.services import SFTTrainer
from TrainingFactory.models import Qwen3Model

# 1. åŠ è½½æ¨¡å‹
model = Qwen3Model("Qwen/Qwen3-4B-Instruct")

# 2. é…ç½®è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    dataset_id="ds_tool_calling_v1",  # æ¥è‡ªDataFactory
    learning_rate=2e-5,
    epochs=3
)

# 3. å¼€å§‹è®­ç»ƒ
trainer.train()
```

### é«˜çº§ç¤ºä¾‹ - GRPOè®­ç»ƒï¼ˆRL-Factoryé£æ ¼ï¼‰

```python
from TrainingFactory.services import GRPOTrainer
from TrainingFactory.tools import MCPTools
from TrainingFactory.rewards import ProcessReward

# 1. é…ç½®ç¯å¢ƒå’Œå·¥å…·
env_config = {
    "environment_id": "env_search_v1",  # æ¥è‡ªEnvironmentFactory
    "tools": MCPTools.load_from_config("tools/search.json"),
    "async_execution": True  # â­ å¼‚æ­¥å¹¶è¡Œ
}

# 2. é…ç½®å¥–åŠ±å‡½æ•°
reward_fn = ProcessReward(
    outcome_weight=1.0,
    step_rewards={
        "correct_tool": 0.5,
        "clear_reasoning": 0.3,
        "efficient_path": 0.2
    }
)

# 3. åˆ›å»ºè®­ç»ƒå™¨
trainer = GRPOTrainer(
    model="Qwen3-4B",
    environment=env_config,
    reward_function=reward_fn,
    num_epochs=100,
    batch_size=8,
    async_rollout=True  # â­ å¼‚æ­¥Rollout
)

# 4. è®­ç»ƒ
trainer.train()
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### å¼‚æ­¥å¹¶è¡Œå·¥å…·è°ƒç”¨

åŸºäºRL-Factoryçš„å®ç°ï¼š

```python
class AsyncRolloutEngine:
    """å¼‚æ­¥Rolloutå¼•æ“"""
    
    def __init__(self, num_workers=8):
        self.workers = [
            ToolWorker() for _ in range(num_workers)
        ]
    
    async def rollout_batch(self, prompts):
        """æ‰¹é‡å¹¶è¡ŒRollout"""
        tasks = []
        for prompt in prompts:
            # å¼‚æ­¥æ‰§è¡Œæ¯ä¸ªpromptçš„rollout
            task = self._async_rollout(prompt)
            tasks.append(task)
        
        # å¹¶è¡Œç­‰å¾…æ‰€æœ‰ç»“æœ
        trajectories = await asyncio.gather(*tasks)
        return trajectories
    
    async def _async_rollout(self, prompt):
        """å•ä¸ªå¼‚æ­¥Rollout"""
        trajectory = []
        current_state = prompt
        
        while not done:
            # æ¨¡å‹ç”Ÿæˆä¸‹ä¸€æ­¥
            action = await self.model.generate(current_state)
            
            # å¼‚æ­¥æ‰§è¡Œå·¥å…·è°ƒç”¨
            if action.is_tool_call:
                result = await self.tool_executor.execute(action.tool)
                trajectory.append((action, result))
            
            current_state = update_state(current_state, action, result)
        
        return trajectory
```

### å·¥å…·è°ƒç”¨ç¼“å­˜

```python
class ToolCache:
    """å·¥å…·è°ƒç”¨ç»“æœç¼“å­˜"""
    
    def __init__(self):
        self.cache = {}
    
    async def execute_with_cache(self, tool_call):
        """å¸¦ç¼“å­˜çš„å·¥å…·æ‰§è¡Œ"""
        cache_key = self._get_cache_key(tool_call)
        
        if cache_key in self.cache:
            # å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›
            return self.cache[cache_key]
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        result = await self.execute_tool(tool_call)
        
        # ç¼“å­˜ç»“æœ
        self.cache[cache_key] = result
        return result
```

---

## ğŸ“Š è®­ç»ƒæ€§èƒ½å¯¹æ¯”

åŸºäºRL-Factoryçš„benchmarkç»“æœï¼š

| æ¨¡å‹ | æ¡†æ¶ | è®­ç»ƒæ—¶é—´ (100 steps) | æ¯æ­¥æ—¶é—´ | å‡†ç¡®åº¦ |
|------|------|---------------------|----------|--------|
| Qwen2.5-3B | Search-R1 | 7.39h | 266s | 0.356 |
| Qwen3-4B | RL-Factory | **5.30h** | **190s** | **0.458** |
| Qwen3-8B | RL-Factory | **5.76h** | **207s** | **0.463** |

**æå‡:**
- âš¡ è®­ç»ƒé€Ÿåº¦ï¼š**2x**ï¼ˆå¼‚æ­¥å·¥å…·è°ƒç”¨ï¼‰
- ğŸ¯ å‡†ç¡®åº¦ï¼š**+28%**ï¼ˆQwen3 + è¿›ç¨‹å¥–åŠ±ï¼‰

---

## ğŸ“ æ”¯æŒçš„è®­ç»ƒç®—æ³•

### 1. SFT (Supervised Fine-Tuning)
- åŸºç¡€çš„ç›‘ç£å­¦ä¹ 
- é€‚åˆï¼šå»ºç«‹åŸºç¡€èƒ½åŠ›

### 2. PPO (Proximal Policy Optimization)
- ç¨³å®šçš„RLç®—æ³•
- é€‚åˆï¼šé€šç”¨Agentè®­ç»ƒ

### 3. DPO (Direct Preference Optimization)
- åŸºäºåå¥½å­¦ä¹ 
- é€‚åˆï¼šå¯¹é½äººç±»åå¥½

### 4. GRPO (Group Relative Policy Optimization) â­
- RL-Factoryæ¨è
- é€‚åˆï¼šTool-callingä¼˜åŒ–
- ç‰¹ç‚¹ï¼šç»„å†…ç›¸å¯¹ä¼˜åŒ–ï¼Œæ›´ç¨³å®š

### 5. RFT (Rejection Fine-Tuning)
- åŸºäºé‡‡æ ·çš„å¾®è°ƒ
- é€‚åˆï¼šæ¨ç†èƒ½åŠ›æå‡

---

## ğŸ”§ MCPå·¥å…·é›†æˆ

æ”¯æŒModel Context Protocol (MCP)å·¥å…·ï¼š

```python
# tools/search_tool.json
{
  "name": "web_search",
  "description": "Search the web",
  "parameters": {
    "query": {
      "type": "string",
      "description": "Search query"
    }
  },
  "mcp_server": "http://mcp-search:8080"
}
```

```python
# ä½¿ç”¨MCPå·¥å…·
from TrainingFactory.tools import MCPTools

tools = MCPTools.load_from_config("tools/search_tool.json")

# è‡ªåŠ¨é›†æˆåˆ°è®­ç»ƒ
trainer = GRPOTrainer(
    model="Qwen3-4B",
    tools=tools,  # è‡ªåŠ¨ä½¿ç”¨MCPåè®®
    ...
)
```

---

## ğŸ“ é…ç½®ç¤ºä¾‹

### GRPOè®­ç»ƒé…ç½® (Qwen3)

```yaml
# configs/qwen3_grpo.yaml
model:
  name: "Qwen/Qwen3-4B-Instruct"
  max_length: 4096

training:
  algorithm: "grpo"
  num_epochs: 100
  batch_size: 8
  learning_rate: 1e-6
  warmup_steps: 10

environment:
  environment_id: "env_search_v1"
  async_execution: true
  num_workers: 8

tools:
  - name: "web_search"
    config: "tools/search.json"
    cache_enabled: true

reward:
  type: "process_reward"
  outcome_weight: 1.0
  step_rewards:
    correct_tool: 0.5
    clear_reasoning: 0.3
    efficient_path: 0.2

optimization:
  async_rollout: true
  distributed_reward: true
  tool_cache: true
```

---

## ğŸ”— ä¸å…¶ä»–Factoryé›†æˆ

### ä»ComputeFactoryè·å–èµ„æº

```python
from ComputeFactory.services import ResourceManager

# ç”³è¯·GPUèµ„æº
manager = ResourceManager()
allocation = manager.allocate_resource(
    pool_type="training",
    resource_spec=ResourceSpec(
        resource_type="gpu",
        count=8,  # 8x A100
        memory_gb=640
    )
)

# ä½¿ç”¨èµ„æºè®­ç»ƒ
trainer = GRPOTrainer(
    allocation_id=allocation.allocation_id,
    ...
)
```

### ä»DataFactoryåŠ è½½æ•°æ®

```python
from DataFactory.services import DatasetManager

# åŠ è½½æ•°æ®é›†
dataset_manager = DatasetManager()
dataset = dataset_manager.get_dataset("ds_tool_calling_v1")

# ç”¨äºè®­ç»ƒ
trainer = SFTTrainer(
    dataset_id=dataset.dataset_id,
    ...
)
```

### ä»EnvironmentFactoryè·å–ç¯å¢ƒ

```python
from EnvironmentFactory.services import ScenarioBuilder

#è·å–ç¯å¢ƒ
scenario = ScenarioBuilder().get_scenario("env_search_v1")

# ç”¨äºRLè®­ç»ƒ
trainer = GRPOTrainer(
    environment_id=scenario.scenario_id,
    ...
)
```

---

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# æµ‹è¯•ç‰¹å®šç®—æ³•
pytest tests/test_grpo.py

# æ€§èƒ½æµ‹è¯•
pytest tests/test_performance.py
```

---

## ğŸ“š ç›¸å…³èµ„æº

- [RL-Factoryé¡¹ç›®](https://github.com/Simple-Efficient/RL-Factory)
- [VeRLæ¡†æ¶](https://github.com/volcengine/veRL)
- [Qwen3æ¨¡å‹](https://github.com/QwenLM/Qwen)
- [MCPåè®®](https://modelcontextprotocol.io/)

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„Base Model
- **Tool-calling**: ä¼˜å…ˆQwen3ï¼ˆæ”¯æŒMCPï¼‰
- **æ¨ç†**: Qwen3æˆ–DeepSeek
- **é€šç”¨**: Llama3

### 2. é…ç½®é«˜æ•ˆè®­ç»ƒ
```python
# âœ… æ¨èé…ç½®
config = {
    "async_rollout": True,      # å¼‚æ­¥Rollout
    "async_tools": True,        # å¼‚æ­¥å·¥å…·è°ƒç”¨
    "tool_cache": True,         # å·¥å…·ç¼“å­˜
    "distributed_reward": True  # åˆ†å¸ƒå¼å¥–åŠ±
}
```

### 3. è®¾è®¡å¥½çš„å¥–åŠ±å‡½æ•°
```python
# âœ… è¿›ç¨‹å¥–åŠ± > çº¯ç»“æœå¥–åŠ±
reward_fn = ProcessReward(
    outcome_weight=1.0,
    step_rewards={...}  # å¥–åŠ±ä¸­é—´æ­¥éª¤
)
```

### 4. ç›‘æ§è®­ç»ƒæŒ‡æ ‡
- è®­ç»ƒé€Ÿåº¦ï¼ˆsteps/secondï¼‰
- å¥–åŠ±æ›²çº¿
- å·¥å…·è°ƒç”¨æˆåŠŸç‡
- æ¨¡å‹å‡†ç¡®åº¦

---

## ğŸš§ æœªæ¥è®¡åˆ’

åŸºäºRL-Factoryçš„è·¯çº¿å›¾ï¼š

- [ ] WebUI for training management
- [ ] æ›´å¤šæ¨¡å‹æ”¯æŒï¼ˆGeminiã€Claudeç­‰ï¼‰
- [ ] å¤šæ¨¡æ€Agentè®­ç»ƒ
- [ ] Androidç¯å¢ƒæ”¯æŒ
- [ ] Process Rewardå®Œå–„
- [ ] MS-SWIFTé›†æˆ

---

**TrainingFactory + RL-Factoryç»éªŒ = é«˜æ•ˆçš„Agentè®­ç»ƒï¼** ğŸ‰
