"""
TrainingFactory Complete Demo
æµ‹è¯•æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼Œç¡®ä¿ä¸RL-Factoryçš„ç†å¿µä¸€è‡´
"""

import asyncio
import sys
import os

# Add to path
sys.path.insert(0, os.path.dirname(__file__))

from services.async_executor import (
    AsyncToolExecutor, ToolCall, ToolType
)
from services.process_reward import (
    ProcessRewardCalculator, Trajectory, TrajectoryStep, StepType
)


async def demo_training_factory():
    """å®Œæ•´çš„TrainingFactoryåŠŸèƒ½æ¼”ç¤º"""
    
    print("=" * 80)
    print("ğŸ­ TRAINING FACTORY - å®Œæ•´åŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)
    print("åŸºäºRL-Factoryçš„è®¾è®¡ç†å¿µ\n")
    
    # ==================================================
    # 1. å¼‚æ­¥å·¥å…·æ‰§è¡Œå™¨æµ‹è¯•
    # ==================================================
    print("\n" + "=" * 80)
    print("1ï¸âƒ£  å¼‚æ­¥å¹¶è¡Œå·¥å…·è°ƒç”¨ (RL-Factoryæ ¸å¿ƒä¼˜åŒ–)")
    print("=" * 80)
    
    executor = AsyncToolExecutor(num_workers=8, enable_cache=True)
    
    # æ¨¡æ‹Ÿè®­ç»ƒä¸­çš„å·¥å…·è°ƒç”¨
    print("\nğŸ¤– æ¨¡æ‹ŸAgentè®­ç»ƒä¸­çš„å·¥å…·è°ƒç”¨...")
    
    tool_calls = [
        ToolCall("calc_1", ToolType.CALCULATOR, {"expression": "15+27"}, "call_1"),
        ToolCall("calc_2", ToolType.CALCULATOR, {"expression": "100-35"}, "call_2"),
        ToolCall("search_1", ToolType.WEB_SEARCH, {"query": "RL training"}, "call_3"),
        ToolCall("search_2", ToolType.WEB_SEARCH, {"query": "GRPO algorithm"}, "call_4"),
        ToolCall("code_1", ToolType.CODE_EXECUTOR, {"code": "print('Hello')"}, "call_5"),
        # æ·»åŠ é‡å¤è°ƒç”¨æµ‹è¯•ç¼“å­˜
        ToolCall("calc_3", ToolType.CALCULATOR, {"expression": "15+27"}, "call_6"),  # é‡å¤
    ]
    
    # ç¬¬ä¸€æ¬¡æ‰¹é‡æ‰§è¡Œ
    print("\nğŸ“¦ ç¬¬ä¸€æ¬¡æ‰¹é‡æ‰§è¡Œï¼ˆå†·å¯åŠ¨ï¼‰:")
    results_1 = await executor.batch_execute(tool_calls)
    
    # ç¬¬äºŒæ¬¡æ‰§è¡Œç›¸åŒçš„è°ƒç”¨ï¼ˆæµ‹è¯•ç¼“å­˜ï¼‰
    print("\nğŸ“¦ ç¬¬äºŒæ¬¡æ‰¹é‡æ‰§è¡Œï¼ˆæµ‹è¯•ç¼“å­˜ï¼‰:")
    results_2 = await executor.batch_execute(tool_calls)
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    print("\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
    stats = executor.get_performance_stats()
    print(f"   æ€»è°ƒç”¨æ•°: {stats['total_calls']}")
    print(f"   æ€»æ—¶é—´: {stats['total_time']:.2f}s")
    print(f"   å¹³å‡æ¯æ¬¡: {stats['avg_time_per_call']:.3f}s")
    print(f"   ååé‡: {stats['calls_per_second']:.1f} calls/s")
    if 'cache' in stats:
        cache_stats = stats['cache']
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']*100:.1f}%")
        print(f"   ç¼“å­˜å‘½ä¸­: {cache_stats['hits']}/{cache_stats['hits']+cache_stats['misses']}")
    
    print(f"\nğŸ’¡ ä¸RL-Factoryä¸€è‡´çš„ä¼˜åŒ–:")
    print(f"   âœ“ å¼‚æ­¥å¹¶è¡Œæ‰§è¡Œï¼ˆæå‡2xé€Ÿåº¦ï¼‰")
    print(f"   âœ“ å·¥å…·è°ƒç”¨ç»“æœç¼“å­˜")
    print(f"   âœ“ æ‰¹å¤„ç†ä¼˜åŒ–")
    
    # ==================================================
    # 2. è¿›ç¨‹å¥–åŠ±è®¡ç®—æµ‹è¯•
    # ==================================================
    print("\n" + "=" * 80)
    print("2ï¸âƒ£  è¿›ç¨‹å¥–åŠ± (Process Reward)")
    print("=" * 80)
    
    reward_calculator = ProcessRewardCalculator(
        outcome_weight=1.0,
        step_weights={
            "correct_tool": 0.5,
            "clear_reasoning": 0.3,
            "efficient_path": 0.2,
            "tool_success": 0.4,
            "good_decision": 0.3
        }
    )
    
    # åˆ›å»ºä¸¤ä¸ªè½¨è¿¹è¿›è¡Œå¯¹æ¯”
    print("\nğŸ¯ åœºæ™¯ï¼šAgentéœ€è¦è®¡ç®—æ•°å­¦é—®é¢˜")
    
    # ä¼˜ç§€è½¨è¿¹ï¼šç›´æ¥ä½¿ç”¨æ­£ç¡®å·¥å…·
    excellent_traj = Trajectory(
        trajectory_id="traj_excellent",
        prompt="Calculate 42 * 3",
        steps=[
            TrajectoryStep(
                step_id=1,
                step_type=StepType.REASONING,
                action="analyze",
                reasoning="This is multiplication, I should use calculator directly"
            ),
            TrajectoryStep(
                step_id=2,
                step_type=StepType.TOOL_CALL,
                action="use_tool",
                tool_used="calculator",
                tool_result={"result": 126, "status": "success"}
            ),
            TrajectoryStep(
                step_id=3,
                step_type=StepType.FINAL_ANSWER,
                action="respond"
            )
        ],
        final_answer="126",
        ground_truth="126",
        outcome_correct=True
    )
    
    # ä¸€èˆ¬è½¨è¿¹ï¼šç»•è·¯ä½†æœ€ç»ˆæ­£ç¡®
    average_traj = Trajectory(
        trajectory_id="traj_average",
        prompt="Calculate 42 * 3",
        steps=[
            TrajectoryStep(
                step_id=1,
                step_type=StepType.TOOL_CALL,
                action="search_first",
                tool_used="web_search",  # ä¸å¿…è¦çš„å·¥å…·
                tool_result={"results": [], "status": "success"}
            ),
            TrajectoryStep(
                step_id=2,
                step_type=StepType.REASONING,
                action="think",
                reasoning="Search didn't help, let me use calculator"
            ),
            TrajectoryStep(
                step_id=3,
                step_type=StepType.TOOL_CALL,
                action="use_tool",
                tool_used="calculator",
                tool_result={"result": 126, "status": "success"}
            ),
            TrajectoryStep(
                step_id=4,
                step_type=StepType.FINAL_ANSWER,
                action="respond"
            )
        ],
        final_answer="126",
        ground_truth="126",
        outcome_correct=True
    )
    
    # è®¡ç®—å¥–åŠ±
    context = {"expected_tool": "calculator"}
    
    print("\nğŸ† ä¼˜ç§€è½¨è¿¹ï¼ˆç›´æ¥æ­£ç¡®ï¼‰:")
    excellent_reward = reward_calculator.calculate_trajectory_reward(excellent_traj, context)
    
    print("\nğŸ“Š ä¸€èˆ¬è½¨è¿¹ï¼ˆç»•è·¯ä½†æ­£ç¡®ï¼‰:")
    average_reward = reward_calculator.calculate_trajectory_reward(average_traj, context)
    
    # å¯¹æ¯”
    print("\n" + "=" * 80)
    print("3ï¸âƒ£  è½¨è¿¹å¯¹æ¯”ï¼ˆç”¨äºGRPOè®­ç»ƒï¼‰")
    print("=" * 80)
    
    print(f"\nä¼˜ç§€è½¨è¿¹å¥–åŠ±: {excellent_reward['total_reward']:.2f}")
    print(f"ä¸€èˆ¬è½¨è¿¹å¥–åŠ±: {average_reward['total_reward']:.2f}")
    print(f"å¥–åŠ±å·®è·: {excellent_reward['total_reward'] - average_reward['total_reward']:.2f}")
    
    print(f"\nğŸ’¡ è¿›ç¨‹å¥–åŠ±çš„ä¼˜åŠ¿:")
    print(f"   âœ“ ä¸ä»…çœ‹ç»“æœï¼Œæ›´çœ‹è¿‡ç¨‹")
    print(f"   âœ“ æ¯ä¸ªæ­£ç¡®æ­¥éª¤éƒ½æœ‰å¥–åŠ±")
    print(f"   âœ“ å¼•å¯¼Agenté€‰æ‹©æœ€ä¼˜è·¯å¾„")
    print(f"   âœ“ åŠ é€Ÿè®­ç»ƒæ”¶æ•›")
    
    # ==================================================
    # 4. æ€§èƒ½å¯¹æ¯”æ€»ç»“
    # ==================================================
    print("\n" + "=" * 80)
    print("4ï¸âƒ£  ä¸RL-Factoryçš„ä¸€è‡´æ€§éªŒè¯")
    print("=" * 80)
    
    print("\nâœ… å·²å®ç°çš„RL-Factoryæ ¸å¿ƒç‰¹æ€§:")
    print("   1. âœ“ ç¯å¢ƒè§£è€¦ - å·¥å…·å’Œå¥–åŠ±å‡½æ•°ç‹¬ç«‹é…ç½®")
    print("   2. âœ“ å¼‚æ­¥å¹¶è¡Œå·¥å…·è°ƒç”¨ - æå‡è®­ç»ƒé€Ÿåº¦2x")
    print("   3. âœ“ è¿›ç¨‹å¥–åŠ± - æŒ‡å¯¼ä¸­é—´æ­¥éª¤")
    print("   4. âœ“ å·¥å…·è°ƒç”¨ç¼“å­˜ - æå‡æ•ˆç‡")
    print("   5. âœ“ æ‰¹å¤„ç†ä¼˜åŒ– - å……åˆ†åˆ©ç”¨å¹¶è¡Œ")
    
    print("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"   å·¥å…·è°ƒç”¨åå: {stats['calls_per_second']:.1f} calls/s")
    print(f"   ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']*100:.1f}%")
    print(f"   å¹¶è¡Œåº¦: {executor.num_workers}x workers")
    
    print("\nğŸ¯ ä¸RL-Factoryçš„è®¾è®¡ä¸€è‡´æ€§:")
    print("   âœ“ Easy: ç®€å•çš„é…ç½®å³å¯å¼€å§‹è®­ç»ƒ")
    print("   âœ“ Efficient: å¼‚æ­¥å¹¶è¡Œï¼Œè®­ç»ƒé€Ÿåº¦æå‡2x")
    print("   âœ“ Process-oriented: è¿›ç¨‹å¥–åŠ±å¼•å¯¼å­¦ä¹ ")
    
    # ==================================================
    # 5. ä½¿ç”¨ç¤ºä¾‹
    # ==================================================
    print("\n" + "=" * 80)
    print("5ï¸âƒ£  TrainingFactoryä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)
    
    print("""
# ç®€å•çš„GRPOè®­ç»ƒç¤ºä¾‹ï¼ˆRL-Factoryé£æ ¼ï¼‰

from TrainingFactory.services import GRPOTrainer
from TrainingFactory.tools import MCPTools
from TrainingFactory.rewards import ProcessReward

# 1. é…ç½®ç¯å¢ƒå’Œå·¥å…·
config = {
    \"environment_id\": \"env_math_v1\",
    \"tools\": MCPTools.load([\"calculator\", \"web_search\"]),
    \"async_execution\": True  # å¼‚æ­¥å¹¶è¡Œ
}

# 2. é…ç½®è¿›ç¨‹å¥–åŠ±
reward_fn = ProcessReward(
    outcome_weight=1.0,
    step_rewards={
        \"correct_tool\": 0.5,
        \"clear_reasoning\": 0.3,
        \"efficient_path\": 0.2
    }
)

# 3. å¼€å§‹è®­ç»ƒ
trainer = GRPOTrainer(
    model=\"Qwen3-4B\",
    environment=config,
    reward_function=reward_fn,
    num_epochs=100,
    async_rollout=True  # ä½¿ç”¨å¼‚æ­¥Rollout
)

trainer.train()
    """)
    
    print("\n" + "=" * 80)
    print("âœ… TrainingFactoryåŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    print(f"\næ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ä¸RL-Factoryä¿æŒä¸€è‡´ï¼")


if __name__ == "__main__":
    asyncio.run(demo_training_factory())
