"""
è¿›ç¨‹å¥–åŠ± (Process Reward)
åŸºäºRL-Factoryçš„è®¾è®¡ï¼šä¸ä»…å¥–åŠ±ç»“æœï¼Œæ›´è¦å¥–åŠ±è¿‡ç¨‹
"""

from typing import List, Dict, Any
from dataclasses import dataclass
from enum import Enum


class StepType(str, Enum):
    """æ­¥éª¤ç±»å‹"""
    TOOL_CALL = "tool_call"
    REASONING = "reasoning"
    DECISION = "decision"
    FINAL_ANSWER = "final_answer"


@dataclass
class TrajectoryStep:
    """è½¨è¿¹ä¸­çš„ä¸€ä¸ªæ­¥éª¤"""
    step_id: int
    step_type: StepType
    action: str
    tool_used: str = None
    tool_result: Any = None
    reasoning: str = ""
    is_correct: bool = False


@dataclass
class Trajectory:
    """å®Œæ•´çš„æ‰§è¡Œè½¨è¿¹"""
    trajectory_id: str
    prompt: str
    steps: List[TrajectoryStep]
    final_answer: Any
    ground_truth: Any = None
    outcome_correct: bool = False


class ProcessRewardCalculator:
    """
    è¿›ç¨‹å¥–åŠ±è®¡ç®—å™¨
    
    RL-Factoryçš„æ ¸å¿ƒç†å¿µï¼š
    - ä¸ä»…å¥–åŠ±æœ€ç»ˆç»“æœ
    - æ¯ä¸ªæ­£ç¡®çš„ä¸­é—´æ­¥éª¤éƒ½ç»™å¥–åŠ±
    - æ›´å¥½åœ°æŒ‡å¯¼Agentçš„å·¥å…·è°ƒç”¨è¡Œä¸º
    """
    
    def __init__(
        self,
        outcome_weight: float = 1.0,
        step_weights: Dict[str, float] = None
    ):
        """
        Args:
            outcome_weight: æœ€ç»ˆç»“æœçš„æƒé‡
            step_weights: å„ç±»æ­¥éª¤çš„å¥–åŠ±æƒé‡
        """
        self.outcome_weight = outcome_weight
        
        # é»˜è®¤æ­¥éª¤å¥–åŠ±æƒé‡
        self.step_weights = step_weights or {
            "correct_tool": 0.5,        # ä½¿ç”¨äº†æ­£ç¡®çš„å·¥å…·
            "clear_reasoning": 0.3,     # æ¨ç†æ¸…æ™°
            "efficient_path": 0.2,      # è·¯å¾„é«˜æ•ˆï¼ˆæ­¥éª¤å°‘ï¼‰
            "tool_success": 0.4,        # å·¥å…·è°ƒç”¨æˆåŠŸ
            "good_decision": 0.3        # å†³ç­–åˆç†
        }
    
    def calculate_step_reward(self, step: TrajectoryStep, context: Dict = None) -> float:
        """
        è®¡ç®—å•ä¸ªæ­¥éª¤çš„å¥–åŠ±
        
        Args:
            step: è½¨è¿¹æ­¥éª¤
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚é¢„æœŸçš„å·¥å…·ç­‰ï¼‰
        
        Returns:
            è¯¥æ­¥éª¤çš„å¥–åŠ±å€¼
        """
        reward = 0.0
        context = context or {}
        
        # 1. å·¥å…·é€‰æ‹©å¥–åŠ±
        if step.step_type == StepType.TOOL_CALL:
            expected_tool = context.get("expected_tool")
            if step.tool_used == expected_tool:
                reward += self.step_weights["correct_tool"]
                print(f"      âœ“ æ­£ç¡®å·¥å…·: +{self.step_weights['correct_tool']}")
            
            # å·¥å…·æ‰§è¡ŒæˆåŠŸ
            if step.tool_result and step.tool_result.get("status") == "success":
                reward += self.step_weights["tool_success"]
                print(f"      âœ“ å·¥å…·æˆåŠŸ: +{self.step_weights['tool_success']}")
        
        # 2. æ¨ç†è´¨é‡å¥–åŠ±
        if step.step_type == StepType.REASONING:
            if step.reasoning and len(step.reasoning) > 10:  # ç®€åŒ–åˆ¤æ–­
                reward += self.step_weights["clear_reasoning"]
                print(f"      âœ“ æ¸…æ™°æ¨ç†: +{self.step_weights['clear_reasoning']}")
        
        # 3. å†³ç­–è´¨é‡å¥–åŠ±
        if step.step_type == StepType.DECISION:
            if step.is_correct:
                reward += self.step_weights["good_decision"]
                print(f"      âœ“ è‰¯å¥½å†³ç­–: +{self.step_weights['good_decision']}")
        
        return reward
    
    def calculate_trajectory_reward(
        self,
        trajectory: Trajectory,
        context: Dict = None
    ) -> Dict[str, float]:
        """
        è®¡ç®—æ•´ä¸ªè½¨è¿¹çš„å¥–åŠ±
        
        è¿™æ˜¯RL-Factoryçš„æ ¸å¿ƒï¼šProcess Reward
        
        Returns:
            åŒ…å«æ€»å¥–åŠ±å’Œè¯¦ç»†å¥–åŠ±çš„å­—å…¸
        """
        context = context or {}
        
        print(f"\nğŸ è®¡ç®—è½¨è¿¹å¥–åŠ±: {trajectory.trajectory_id}")
        
        # 1. è®¡ç®—æ¯ä¸ªæ­¥éª¤çš„å¥–åŠ±
        step_rewards = []
        total_step_reward = 0.0
        
        for i, step in enumerate(trajectory.steps):
            print(f"   æ­¥éª¤ {i+1}/{len(trajectory.steps)}: {step.step_type}")
            step_reward = self.calculate_step_reward(step, context)
            step_rewards.append(step_reward)
            total_step_reward += step_reward
        
        # 2. è®¡ç®—ç»“æœå¥–åŠ±
        outcome_reward = 0.0
        if trajectory.outcome_correct:
            outcome_reward = self.outcome_weight
            print(f"   âœ“ ç»“æœæ­£ç¡®: +{outcome_reward}")
        else:
            outcome_reward = -self.outcome_weight * 0.5  # é”™è¯¯ç»“æœæƒ©ç½š
            print(f"   âœ— ç»“æœé”™è¯¯: {outcome_reward}")
        
        # 3. æ•ˆç‡å¥–åŠ±ï¼ˆæ­¥éª¤è¶Šå°‘è¶Šå¥½ï¼‰
        efficiency_reward = 0.0
        if len(trajectory.steps) <= 3:  # å°‘äº3æ­¥å¾ˆé«˜æ•ˆ
            efficiency_reward = self.step_weights["efficient_path"]
            print(f"   âœ“ é«˜æ•ˆè·¯å¾„: +{efficiency_reward}")
        
        # 4. æ€»å¥–åŠ±
        total_reward = total_step_reward + outcome_reward + efficiency_reward
        
        print(f"   ğŸ“Š æ€»å¥–åŠ±: {total_reward:.2f}")
        print(f"      - æ­¥éª¤å¥–åŠ±: {total_step_reward:.2f}")
        print(f"      - ç»“æœå¥–åŠ±: {outcome_reward:.2f}")
        print(f"      - æ•ˆç‡å¥–åŠ±: {efficiency_reward:.2f}")
        
        return {
            "total_reward": total_reward,
            "step_reward": total_step_reward,
            "outcome_reward": outcome_reward,
            "efficiency_reward": efficiency_reward,
            "step_rewards": step_rewards,
            "breakdown": {
                "steps": total_step_reward,
                "outcome": outcome_reward,
                "efficiency": efficiency_reward
            }
        }
    
    def compare_trajectories(
        self,
        trajectories: List[Trajectory],
        context: Dict = None
    ) -> List[Dict]:
        """
        æ¯”è¾ƒå¤šä¸ªè½¨è¿¹ï¼Œè®¡ç®—ç›¸å¯¹å¥–åŠ±
        ç”¨äºGRPOç­‰ç®—æ³•
        """
        results = []
        
        for traj in trajectories:
            reward_info = self.calculate_trajectory_reward(traj, context)
            results.append({
                "trajectory_id": traj.trajectory_id,
                "total_reward": reward_info["total_reward"],
                "details": reward_info
            })
        
        # æŒ‰å¥–åŠ±æ’åº
        results.sort(key=lambda x: x["total_reward"], reverse=True)
        
        return results


# Demo
def demo_process_reward():
    """æ¼”ç¤ºè¿›ç¨‹å¥–åŠ±è®¡ç®—"""
    print("=" * 60)
    print("è¿›ç¨‹å¥–åŠ± (Process Reward) Demo")
    print("=" * 60)
    
    # åˆ›å»ºå¥–åŠ±è®¡ç®—å™¨
    calculator = ProcessRewardCalculator(
        outcome_weight=1.0,
        step_weights={
            "correct_tool": 0.5,
            "clear_reasoning": 0.3,
            "efficient_path": 0.2,
            "tool_success": 0.4,
            "good_decision": 0.3
        }
    )
    
    # åœºæ™¯ï¼šAgentéœ€è¦è®¡ç®—"2+3"
    
    # è½¨è¿¹1ï¼šå¥½çš„è½¨è¿¹ï¼ˆç›´æ¥ä½¿ç”¨è®¡ç®—å™¨ï¼‰
    good_trajectory = Trajectory(
        trajectory_id="traj_good",
        prompt="What is 2+3?",
        steps=[
            TrajectoryStep(
                step_id=1,
                step_type=StepType.REASONING,
                action="think",
                reasoning="This is a simple arithmetic problem, I should use calculator"
            ),
            TrajectoryStep(
                step_id=2,
                step_type=StepType.TOOL_CALL,
                action="call_tool",
                tool_used="calculator",
                tool_result={"result": 5, "status": "success"}
            ),
            TrajectoryStep(
                step_id=3,
                step_type=StepType.FINAL_ANSWER,
                action="answer"
            )
        ],
        final_answer="5",
        ground_truth="5",
        outcome_correct=True
    )
    
    # è½¨è¿¹2ï¼šå·®çš„è½¨è¿¹ï¼ˆå…ˆæœç´¢å†è®¡ç®—ï¼Œæ­¥éª¤å¤šï¼‰
    bad_trajectory = Trajectory(
        trajectory_id="traj_bad",
        prompt="What is 2+3?",
        steps=[
            TrajectoryStep(
                step_id=1,
                step_type=StepType.TOOL_CALL,
                action="call_tool",
                tool_used="web_search",  # é”™è¯¯çš„å·¥å…·
                tool_result={"results": [], "status": "success"}
            ),
            TrajectoryStep(
                step_id=2,
                step_type=StepType.REASONING,
                action="think",
                reasoning="Search didn't help, let me try calculator"
            ),
            TrajectoryStep(
                step_id=3,
                step_type=StepType.TOOL_CALL,
                action="call_tool",
                tool_used="calculator",
                tool_result={"result": 5, "status": "success"}
            ),
            TrajectoryStep(
                step_id=4,
                step_type=StepType.FINAL_ANSWER,
                action="answer"
            )
        ],
        final_answer="5",
        ground_truth="5",
        outcome_correct=True
    )
    
    # è®¡ç®—å¥–åŠ±
    context = {"expected_tool": "calculator"}
    
    print("\n" + "=" * 60)
    print("å¥½çš„è½¨è¿¹ï¼ˆç›´æ¥ä½¿ç”¨æ­£ç¡®å·¥å…·ï¼‰")
    print("=" * 60)
    good_reward = calculator.calculate_trajectory_reward(good_trajectory, context)
    
    print("\n" + "=" * 60)
    print("å·®çš„è½¨è¿¹ï¼ˆç»•è·¯ï¼Œå…ˆç”¨é”™è¯¯å·¥å…·ï¼‰")
    print("=" * 60)
    bad_reward = calculator.calculate_trajectory_reward(bad_trajectory, context)
    
    print("\n" + "=" * 60)
    print("å¯¹æ¯”ç»“æœ")
    print("=" * 60)
    print(f"å¥½è½¨è¿¹å¥–åŠ±: {good_reward['total_reward']:.2f}")
    print(f"å·®è½¨è¿¹å¥–åŠ±: {bad_reward['total_reward']:.2f}")
    print(f"å¥–åŠ±å·®è·: {good_reward['total_reward'] - bad_reward['total_reward']:.2f}")
    print("\nğŸ’¡ å¥½çš„è½¨è¿¹è·å¾—æ›´é«˜å¥–åŠ±ï¼Œå› ä¸ºï¼š")
    print("   1. ç›´æ¥ä½¿ç”¨äº†æ­£ç¡®çš„å·¥å…·")
    print("   2. æ­¥éª¤æ›´å°‘ï¼ˆæ›´é«˜æ•ˆï¼‰")
    print("   3. æ¨ç†æ¸…æ™°")


if __name__ == "__main__":
    demo_process_reward()
